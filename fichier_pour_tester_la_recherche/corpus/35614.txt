 Information theory is a branch of applied mathematics and electrical engineering Information theory measures the amount of information in data that could have more than one value In its most common use information theory finds physical and mathematics mathematical limits on the amounts of data in data compression and data communication Data compression and data communication are statistics statistical because they guess unknown values The amount of information in data measures how easily it is guessed by a person who does not know its value A key measure in information theory is information entropy entropy Entropy quantifies the amount of uncertainty involved in the value of a random variable or the outcome of a Stochastic process random process For example identifying the outcome of a fair coin flip Some other important measures in information theory are mutual information channel capacity error exponent s and relative entropy 