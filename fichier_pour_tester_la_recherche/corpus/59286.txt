A Cache algorithm is an algorithm used to manage a cache or group of data When the cache is full it decides which item should be deleted from the cache The word hit rate describes how often a request can be served from the cache The term latency describes for how long a cached item can be obtained Cache alorithms are a trade off between hit rate and latency Least Recently Used deletes the least recently used items first This algorithm requires keeping track of what was used when This is expensive if one wants to make sure the algorithm always discards the least recently used item General implementations of this technique require to keep age bits for cache lines and track the Least Recently Used cache line based on age bits In such implementation every time a cache line is used the age of all other cache lines changes LRU is actually Page replacement algorithm Variants on LRU a family of caching algorithms with members including Q by Theodore Johnson and Dennis Shasha and LRU K by Pat O Neil Betty O Neil and Gerhard Weikum Most Recently Used This algorithm deletes the most recently used items first This caching mechanism is commonly used for database memory caches Pseudo LRU There are certain cases where LRU is very difficult to implement In such cases it may be enough to know that in most cases one of the least recently used items is deleted The PLRU algorithm can be used there because it only needs one bit per cache item to work Least Frequently Used LFU counts how often an item is needed Those that are used least often are discarded first Adaptive Replacement Cache constantly balances between LRU and LFU to improve combined result Multi Queue Other things to consider Various algorithms also exist to maintain cache coherency This applies only to situation where multiple independent caches are used for the same data 