 Information entropy is a concept from information theory It tells how much information there is in an event In general the more uncertain or random the event is the more information it will contain The concept of information entropy was created by mathematician Claude Shannon It has applications in many areas including lossless data compression statistical inference cryptography and sometimes in other disciplines as biology physics or machine learning The information gain is a measure of the probability with which a certain result is expected to happen In the context of a coin flip with a probability the entropy is the highest value of It does not involve information gain because it does not incline towards a specific result more than the other If there is a probability that a result will occur the entropy is 