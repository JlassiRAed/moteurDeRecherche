 Caching is a term used in computer science A cache IPA pronounced cash cite webFor this reason it is much cheaper to simply use the copy of the data from the cache Put differently a cache is a temporary storage area that has copies of data that is used often When a copy of the data is in this cache it is faster to use this copy rather than re fetching or re calculating the original data This will make the average time needed to access the data shorter A buffer is very similar to a cache It is different in that the client accessing the data in a buffer knows there is a buffer the buffer is managed by the application With a cache the client accessing the data need not be aware there is a cache Typical computer applications access data in very similar ways Suppose the data is structured into blocks which can be accessed individually When an application accesses a block it is also very likely to access a block that is close to the original block This is known as locality of reference There are different kinds of such wikt locality locality Locality of reference is one of the reasons why caches work well in many areas of computing In order to work well caches are small compared to the whole amount of data The bigger the cache the longer it takes to lookup an entry Bigger caches are also more expensive to build 